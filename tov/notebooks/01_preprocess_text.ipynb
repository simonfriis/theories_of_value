{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "    - Remove quoted text? Or maybe not, since it provides context.\n",
    "    - Find common acronyms and synonyms. E.g. sm64, diablo 2 -> diablo_ii\n",
    "    - encode_decode function removes non-english characters. How many game names are in kanji?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../')\n",
    "\n",
    "import ujson as json\n",
    "import pickle\n",
    "import multiprocess as mp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import ftfy\n",
    "from textacy import preprocessing as textacy_preproc\n",
    "from src.utils.reddit_data import RedditData\n",
    "from src.utils import text_preprocessing\n",
    "from src.utils.resources import CONTRACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create game tokens\n",
    "\n",
    "We want to convert game names to single tokens. E.g. \"Super Mario 64\" to \"super_mario_64\". To identify games, we will use a list of all games on Twitch.\n",
    "\n",
    "First, we will preprocess the game names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>normalized</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Breakfree</td>\n",
       "      <td>breakfree</td>\n",
       "      <td>breakfree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hyperballoid Deluxe: Survival Pack</td>\n",
       "      <td>hyperballoid deluxe survival pack</td>\n",
       "      <td>hyperballoid_deluxe_survival_pack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bass Avenger</td>\n",
       "      <td>bass avenger</td>\n",
       "      <td>bass_avenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Chessmaster 2000</td>\n",
       "      <td>the chessmaster 2000</td>\n",
       "      <td>the_chessmaster_2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Desert Strike: Return to the Gulf</td>\n",
       "      <td>desert strike return to the gulf</td>\n",
       "      <td>desert_strike_return_to_the_gulf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98319</th>\n",
       "      <td>2147110739</td>\n",
       "      <td>Planet Rescue: Wildlife Vet</td>\n",
       "      <td>planet rescue wildlife vet</td>\n",
       "      <td>planet_rescue_wildlife_vet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98320</th>\n",
       "      <td>2147114054</td>\n",
       "      <td>Bubble Breaking</td>\n",
       "      <td>bubble breaking</td>\n",
       "      <td>bubble_breaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98321</th>\n",
       "      <td>2147244070</td>\n",
       "      <td>Big Cock Simulator</td>\n",
       "      <td>big cock simulator</td>\n",
       "      <td>big_cock_simulator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98322</th>\n",
       "      <td>2147344492</td>\n",
       "      <td>Mr. Bean's Wacky World</td>\n",
       "      <td>mr bean s wacky world</td>\n",
       "      <td>mr_bean_s_wacky_world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98323</th>\n",
       "      <td>2147350045</td>\n",
       "      <td>Galaga Arrangement</td>\n",
       "      <td>galaga arrangement</td>\n",
       "      <td>galaga_arrangement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93462 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                name  \\\n",
       "1               1                           Breakfree   \n",
       "2               2  Hyperballoid Deluxe: Survival Pack   \n",
       "3               3                        Bass Avenger   \n",
       "4               4                The Chessmaster 2000   \n",
       "5               5   Desert Strike: Return to the Gulf   \n",
       "...           ...                                 ...   \n",
       "98319  2147110739         Planet Rescue: Wildlife Vet   \n",
       "98320  2147114054                     Bubble Breaking   \n",
       "98321  2147244070                  Big Cock Simulator   \n",
       "98322  2147344492              Mr. Bean's Wacky World   \n",
       "98323  2147350045                  Galaga Arrangement   \n",
       "\n",
       "                              normalized                              token  \n",
       "1                              breakfree                          breakfree  \n",
       "2      hyperballoid deluxe survival pack  hyperballoid_deluxe_survival_pack  \n",
       "3                           bass avenger                       bass_avenger  \n",
       "4                   the chessmaster 2000               the_chessmaster_2000  \n",
       "5       desert strike return to the gulf   desert_strike_return_to_the_gulf  \n",
       "...                                  ...                                ...  \n",
       "98319         planet rescue wildlife vet         planet_rescue_wildlife_vet  \n",
       "98320                    bubble breaking                    bubble_breaking  \n",
       "98321                 big cock simulator                 big_cock_simulator  \n",
       "98322              mr bean s wacky world              mr_bean_s_wacky_world  \n",
       "98323                 galaga arrangement                 galaga_arrangement  \n",
       "\n",
       "[93462 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a preprocessing pipeline\n",
    "preproc = textacy_preproc.make_pipeline(\n",
    "    text_preprocessing.to_lower,\n",
    "    ftfy.fix_text,\n",
    "    textacy_preproc.normalize.unicode,\n",
    "    textacy_preproc.normalize.quotation_marks,\n",
    "    textacy_preproc.remove.accents,\n",
    "    textacy_preproc.remove.punctuation,\n",
    "    textacy_preproc.replace.currency_symbols,\n",
    "    text_preprocessing.replace_contractions,\n",
    "    text_preprocessing.encode_decode,\n",
    "    textacy_preproc.normalize.whitespace\n",
    ")\n",
    "\n",
    "# Load games\n",
    "games_file = r'../data/interim/game_ids_and_names.jsonl'\n",
    "games = pd.read_json(games_file, lines = True)\n",
    "\n",
    "# Filter out games missing names\n",
    "games = games[games['name'].notnull()]\n",
    "\n",
    "# Preprocess game names\n",
    "games['normalized'] = [preproc(game) for game in games['name']]\n",
    "games['token'] = ['_'.join(nltk.word_tokenize(str(game))) for game in games['normalized']]\n",
    "\n",
    "# Save the normalized tokens. These will be used later for calculating similarities.\n",
    "games.to_json('../data/interim/game_tokens.jsonl', orient='records', lines=True)\n",
    "\n",
    "games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1,726 normalized game names that are duplicated. Some of these appear to be actual duplicates, but this won't affect the final results since we are going to look up \"similarity\" using the game ids and we want the embedding scores to be the same if different game ids refer to the same game. However, many end up being duplicates because of punctuation. E.g. Zombies. Zombies! and Zombies!!! might refer to different games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>normalized</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59048</th>\n",
       "      <td>17741681</td>\n",
       "      <td>人工灭绝</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87186</th>\n",
       "      <td>1543328458</td>\n",
       "      <td>东方华彩乱战</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90395</th>\n",
       "      <td>1711326006</td>\n",
       "      <td>觅长生</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88921</th>\n",
       "      <td>1631367032</td>\n",
       "      <td>飄流幻境</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90304</th>\n",
       "      <td>1706136320</td>\n",
       "      <td>ПИР: Книга Первая «Семейные узы»</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97195</th>\n",
       "      <td>2084047683</td>\n",
       "      <td>Zombies!</td>\n",
       "      <td>zombies</td>\n",
       "      <td>zombies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19598</th>\n",
       "      <td>23811</td>\n",
       "      <td>Zoom</td>\n",
       "      <td>zoom</td>\n",
       "      <td>zoom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12364</th>\n",
       "      <td>14763</td>\n",
       "      <td>Zoom!</td>\n",
       "      <td>zoom</td>\n",
       "      <td>zoom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96835</th>\n",
       "      <td>2063716522</td>\n",
       "      <td>魔物娘と不思議な冒険～力の宝珠と帰還の塔～</td>\n",
       "      <td>~~</td>\n",
       "      <td>~~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70462</th>\n",
       "      <td>633715586</td>\n",
       "      <td>俺の嫁 ～あなただけの花嫁～</td>\n",
       "      <td>~~</td>\n",
       "      <td>~~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                              name normalized    token\n",
       "59048    17741681                              人工灭绝                    \n",
       "87186  1543328458                            东方华彩乱战                    \n",
       "90395  1711326006                               觅长生                    \n",
       "88921  1631367032                              飄流幻境                    \n",
       "90304  1706136320  ПИР: Книга Первая «Семейные узы»                    \n",
       "...           ...                               ...        ...      ...\n",
       "97195  2084047683                          Zombies!    zombies  zombies\n",
       "19598       23811                              Zoom       zoom     zoom\n",
       "12364       14763                             Zoom!       zoom     zoom\n",
       "96835  2063716522             魔物娘と不思議な冒険～力の宝珠と帰還の塔～         ~~       ~~\n",
       "70462   633715586                    俺の嫁 ～あなただけの花嫁～         ~~       ~~\n",
       "\n",
       "[1940 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "dupes = games[games.duplicated(subset = 'token', keep = False)].sort_values('token')\n",
    "dupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the multi-word tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem: the MWE doesn't seem to capture some games. e.g. dark_souls_2 instead of dark_souls 2\n",
    "# does this have to do with the order in which the MWE tokenizer checks for matches?\n",
    "# sort games from most to least spaces to see if this makes longer MWEs get tokenized first.\n",
    "\n",
    "unique_games = list(set(games['token']))\n",
    "unique_games.sort(key=len)\n",
    "unique_games.reverse()\n",
    "\n",
    "game_tokens = [tok.split('_') for tok in unique_games]\n",
    "\n",
    "mwe_tokenizer = MWETokenizer(game_tokens)\n",
    "\n",
    "# Add extra phrases to MWE tokenizer\n",
    "contractions = list(CONTRACTIONS.values())\n",
    "other_phrases = ['any%', '100%', 'speedrun com', 'twitch com', 'twitch tv', 'speed run', 'speed runs', 'speed running', 'e sport', 'e sports']\n",
    "\n",
    "for phrase in contractions + other_phrases:\n",
    "    phrase = nltk.word_tokenize(phrase)\n",
    "    mwe_tokenizer.add_mwe(phrase)\n",
    "\n",
    "# Save MWE tokenizer\n",
    "with open(r'../models/mwe_tokenizer.pickle', 'wb') as fh:\n",
    "   pickle.dump(mwe_tokenizer, fh)\n",
    "\n",
    "# Check the MWE:\n",
    "# games['mwe'] = [' '.join(mwe_tokenizer.tokenize(nltk.word_tokenize(str(game)))) for game in games['normalized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Reddit posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below repeats a lot of info to get multiprocessing to work in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5816250, 6217561, 5959294, 5625476, 5603319, 5797814, 5743643, 6027611, 6026264, 5707875, 5784793, 6154251, 5482489, 5875928, 5715465, 5604187, 5511348, 5884667, 5727056, 6016020, 5830300, 434552, 459334, 433477, 454558, 447906, 422165, 462622, 437270, 448749, 426112, 426201, 457442, 439547, 442071, 455403, 431036, 440270, 437862, 462397, 442543, 444844]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    posts = RedditData(\n",
    "        comments_path='../data/raw/reddit/comments',\n",
    "        submissions_path='../data/raw/reddit/submissions'\n",
    "    )\n",
    "\n",
    "    def process_file(f):\n",
    "        from pathlib import Path\n",
    "        import gzip\n",
    "        import json\n",
    "        import pickle\n",
    "        import ftfy\n",
    "        from textacy import preprocessing as textacy_preproc\n",
    "        from src.utils.reddit_data import RedditData\n",
    "        from src.utils import text_preprocessing\n",
    "        from tqdm.notebook import tqdm\n",
    "        from html import unescape\n",
    "\n",
    "        mwe_tokenizer = pickle.load(open(r'../models/mwe_tokenizer.pickle', 'rb'))\n",
    "\n",
    "        def merge_mwes(text: str) -> str:\n",
    "            \"\"\"Concatenate multiword expressions using underscore\"\"\"\n",
    "\n",
    "            return text_preprocessing.merge_mwes(text, mwe_tokenizer)\n",
    "\n",
    "        preproc = textacy_preproc.make_pipeline(\n",
    "            text_preprocessing.to_lower,\n",
    "            text_preprocessing.replace_special_characters,\n",
    "            ftfy.fix_text,\n",
    "            textacy_preproc.normalize.unicode,\n",
    "            textacy_preproc.normalize.quotation_marks,\n",
    "            textacy_preproc.remove.accents,\n",
    "            text_preprocessing.replace_markdown_url,\n",
    "            text_preprocessing.replace_urls,\n",
    "            text_preprocessing.normalize_domains,\n",
    "            text_preprocessing.replace_times,\n",
    "            text_preprocessing.replace_money,\n",
    "            text_preprocessing.replace_emojis,\n",
    "            text_preprocessing.replace_currency_symbols,\n",
    "            text_preprocessing.normalize_subreddit,\n",
    "            text_preprocessing.replace_contractions,\n",
    "            text_preprocessing.remove_punctuation,\n",
    "            merge_mwes,\n",
    "            text_preprocessing.replace_numbers,\n",
    "            text_preprocessing.encode_decode,\n",
    "            text_preprocessing.replace_repeating_tokens,\n",
    "            textacy_preproc.normalize.whitespace\n",
    "        )\n",
    "\n",
    "        posts = RedditData(\n",
    "            comments_path='../data/raw/reddit/comments',\n",
    "            submissions_path='../data/raw/reddit/submissions'\n",
    "        )\n",
    "\n",
    "        file_name = f['file']\n",
    "        post_type = f['post_type']\n",
    "        parsed_posts = posts.read_file(file_name, post_type)\n",
    "        outpath = Path(f'../data/interim/reddit_{post_type}_{file_name.stem}.jsonl.gz')\n",
    "        lines_out = 0\n",
    "\n",
    "        with gzip.open(outpath, 'wt', encoding='utf-8') as file_out:\n",
    "            for post in parsed_posts:\n",
    "                post['text'] = preproc(post['text'])\n",
    "\n",
    "                if text_preprocessing.filter_post(post['text']):\n",
    "                    continue\n",
    "\n",
    "                line = json.dumps(post)\n",
    "                file_out.write(line + '\\n')\n",
    "\n",
    "                lines_out += 1\n",
    "\n",
    "        return lines_out\n",
    "\n",
    "    n_cpu = mp.cpu_count() - 2\n",
    "    with mp.Pool(processes=n_cpu) as pool:\n",
    "        result = pool.map(process_file, posts.all_files)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34609</th>\n",
       "      <td>b'gon na'</td>\n",
       "      <td>11.922062</td>\n",
       "      <td>1223959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88287</th>\n",
       "      <td>b'wan na'</td>\n",
       "      <td>11.765868</td>\n",
       "      <td>384263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7218</th>\n",
       "      <td>b'assassins creed'</td>\n",
       "      <td>55.671868</td>\n",
       "      <td>298150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65502</th>\n",
       "      <td>b'questions or objections'</td>\n",
       "      <td>15.444972</td>\n",
       "      <td>211469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35744</th>\n",
       "      <td>b'gta v'</td>\n",
       "      <td>12.540109</td>\n",
       "      <td>202261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23374</th>\n",
       "      <td>b'ding dong'</td>\n",
       "      <td>271.477877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55643</th>\n",
       "      <td>b'navy seals'</td>\n",
       "      <td>440.834743</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90848</th>\n",
       "      <td>b'xclass hardware'</td>\n",
       "      <td>10.814350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>b'advisory stickers'</td>\n",
       "      <td>10.435145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73995</th>\n",
       "      <td>b'silverkoch media'</td>\n",
       "      <td>27.168785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92648 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           phrase       score    count\n",
       "34609                   b'gon na'   11.922062  1223959\n",
       "88287                   b'wan na'   11.765868   384263\n",
       "7218           b'assassins creed'   55.671868   298150\n",
       "65502  b'questions or objections'   15.444972   211469\n",
       "35744                    b'gta v'   12.540109   202261\n",
       "...                           ...         ...      ...\n",
       "23374                b'ding dong'  271.477877        1\n",
       "55643               b'navy seals'  440.834743        1\n",
       "90848          b'xclass hardware'   10.814350        1\n",
       "3864         b'advisory stickers'   10.435145        1\n",
       "73995         b'silverkoch media'   27.168785        1\n",
       "\n",
       "[92648 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import pandas as pd\n",
    "from src.utils.resources import ENGLISH_CONNECTOR_WORDS\n",
    "from src.utils.text_preprocessing import GensimCorpus\n",
    "from pathlib import Path\n",
    "\n",
    "# corpus = GensimCorpus(r'../data/interim/all_reddit_posts.jsonl.gz', sample=10000)\n",
    "# corpus = GensimCorpus(r'../data/processed/esports_speedrun_reddit_posts.txt')\n",
    "\n",
    "files = [f for f in Path(r'../data/interim/').glob('reddit_*.jsonl.gz')]\n",
    "corpus = GensimCorpus(files)\n",
    "\n",
    "phrases = Phrases(corpus, min_count=10, common_terms=ENGLISH_CONNECTOR_WORDS)\n",
    "phrases.save(r'../models/phrases_jan_4.bg')\n",
    "\n",
    "phrase_export = phrases.export_phrases(corpus)\n",
    "bigrams_df = pd.DataFrame(phrase_export)\n",
    "bigrams_df.columns = ['phrase', 'score']\n",
    "bigram_counts = bigrams_df.groupby(['phrase', 'score'])['phrase'].count().reset_index(name='count')\n",
    "bigram_counts.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts.to_csv(r'../models/bigram_phrase_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = pd.read_csv(r'../models/bigram_phrase_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram_counts.sort_values('count', ascending=False).head(100)\n",
    "# x = bigram_counts.iloc[263].phrase\n",
    "x = b'\\xc2\\xb0'\n",
    "x.decode('unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.preprocessing\n",
    "# Filter out phrases that are uncommon or have low score\n",
    "\n",
    "# Filter out anything with _SUBREDDIT_ in it\n",
    "\n",
    "\n",
    "def ignore(text: str) -> bool:\n",
    "    ignore = {'_SUBREDDIT_', '_EMOJI_', r'\\x'}\n",
    "\n",
    "    for token in ignore:\n",
    "        if token in text:\n",
    "            return True\n",
    "\n",
    "    text = textacy.preprocessing.remove.punctuation(text)\n",
    "    text = textacy.preprocessing.normalize.whitespace(text)\n",
    "    \n",
    "    if len(text) <= 3:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "for i, phrase in enumerate(bigram_counts['phrase'].tolist()):\n",
    "    if ignore(phrase):\n",
    "        print(phrase)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "256752c59ca53ce234fd9f70f020053ae5087ee31ea05e2a731da9a92fd8f7f9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('streamers-word2vec-VwsrMEeL-py3.9': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
